# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.


# The configuration file needs to define the sources, 
# the channels and the sinks.
# Sources, channels and sinks are defined per agent, 
# in this case called 'agent'

agent1.sources = taildir_src
agent1.channels = fileChannel
agent1.sinks = hdfs_sink

# For each one of the sources, the type is defined
agent1.sources.taildir_src.type = TAILDIR

agent1.sources.taildir_src.positionFile = /root/dw-course/streaming-etl/taildir_position.json
agent1.sources.taildir_src.filegroups = f1 f2
agent1.sources.taildir_src.filegroups.f1 = /root/dw-course/streaming-etl/logs/daily/.*log
agent1.sources.taildir_src.headers.f1.headerKey1 = app1
agent1.sources.taildir_src.filegroups.f2 = /root/dw-course/streaming-etl/logs/daily2/.*log
agent1.sources.taildir_src.headers.f2.headerKey1 = app2
agent1.sources.taildir_src.headers.f2.headerKey2 = app2-2
agent1.sources.taildir_src.fileHeader = true
agent1.sources.taildir_src.fileHeaderKey  = file

# The channel can be defined as follows.
agent1.sources.taildir_src.channels = fileChannel

# Each sink's type must be defined
agent1.sinks.hdfs_sink.type = hdfs
agent1.sinks.hdfs_sink.hdfs.path = hdfs://starl:9000/user/dw-course/user-action-daily
# 演示header的用处
#agent1.sinks.hdfs_sink.hdfs.path = hdfs://master-dev:9999/user/hadoop-twq/dw-course/user-action-daily/%{headerKey1}
agent1.sinks.hdfs_sink.hdfs.useLocalTimeStamp = true 
# 上面匹配时间的话，需要打开这个开关，这个配置默认是false

agent1.sinks.hdfs_sink.hdfs.round = true
agent1.sinks.hdfs_sink.hdfs.roundValue = 10
agent1.sinks.hdfs_sink.hdfs.roundUnit = minute
# For example, an event with timestamp 11:54:34 AM, June 12, 2012 will cause the hdfs path to become /flume/events/2012-06-12/1150/00.

agent1.sinks.hdfs_sink.hdfs.filePrefix = events 
# 表示文件名的前缀，默认为FlumeData

agent1.sinks.hdfs_sink.hdfs.fileSuffix = .txt 
# 表示文件名的后缀，默认没有后缀

agent1.sinks.hdfs_sink.hdfs.idleTimeout = 0 
# 如果临时文件在这个时间内没有写数据的话，则将这个文件关闭掉，默认等于0，表示不关闭闲文件

agent1.sinks.hdfs_sink.hdfs.batchSize = 100 
# 表示数据每次flush到hdfs的event数，默认就是100

agent1.sinks.hdfs_sink.hdfs.inUsePrefix = events-tmp 
# 表示写临时文件的文件名的前缀，默认没有值

agent1.sinks.hdfs_sink.hdfs.inUseSuffix = .tmp 
# 表示写临时文件的文件名的后缀，默认就是.tmp

agent1.sinks.hdfs_sink.hdfs.rollInterval =30 
# 表示达到了30秒就将临时文件合并成最终文件，默认就是30秒，如果是0的话则表示不按照时间的多少来合并临时文件

agent1.sinks.hdfs_sink.hdfs.rollSize =0 
# 表示文件的大小达到了1024 bytes，则将临时文件合并成最终文件，默认就是1024字节，如0则表示不按照文件大小合并

agent1.sinks.hdfs_sink.hdfs.rollCount =0 
# 表示写入event的数量达到10条的话，则将临时文件合并成最终文件，默认就是10条，如0则表示不按照文件大小合并

agent1.sinks.hdfs_sink.hdfs.fileType = DataStream  
# 表示没有压缩的text文件，默认是SequenceFile，还有CompressedStream表示压缩的文本文件，需要设置hdfs.codeC

# agent1.sinks.hdfs_sink.hdfs.codeC = snappy 
# 可以为：gzip, bzip2, lzo, lzop

#Specify the channel the sink should use
agent1.sinks.hdfs_sink.channel = fileChannel

# 使用File Channel.
agent1.channels.fileChannel.type = file
# 存储将event发送到sink的位置信息，默认每隔30秒做一次checkpoint
agent1.channels.fileChannel.checkpointDir = ~/.flume/file-channel/checkpoint
agent1.channels.fileChannel.checkpointInterval = 30000
# event存储的地方，可以用“,”隔开，配置多个存储文件目录(放在多个磁盘上以提高性能)
agent1.channels.fileChannel.dataDirs  = ~/.flume/file-channel/data,~/.flume/file-channel/data1